# Transformer_encoder

The goal of this project was to create a transformer encoder based on the infamous "Attention is all you need" paper 
[Vaswani, A. et al. (2017) Attention is all you need, arXiv.org. Available at: https://arxiv.org/abs/1706.03762 ] (Accessed: 25 June 2023).

I make the entire architecture using only Numpy and Pytorch with aims to replicate the following parts of a transformer encoder below:

![img](https://github.com/RJCE0/Transformer_encoder/blob/main/images/The-Transformer-encoder-diagram.jpg)
