{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a transformer encoder archetecture using the Transformer model paper. We will use the following diagram to design the architecture:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/The-Transformer-encoder-diagram.jpg\" >\n",
    "    <figcaption> Fig. 1: The Transformer encoder architecture. </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key concept we are interested in captioning for the transformer is attention. This is allows for the transformer to attend to different parts of another sequence when making predictions. For large language models (LLM), it is this very important aspect that is crucial for effective performance with language tasks such as sentiment analysis and text summarisation. \n",
    "\n",
    "We will start by building single-headed attention using the scaled dot product and then later utilise this for multi-headed attention:\n",
    "\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/single-head-attention.jpeg\" >\n",
    "    <figcaption> Fig. 2: Single-headed attention architecture. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate the single attention head values and attention.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\"\"\"\n",
    "    \n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    #print(f\"scaled size: {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        #print(f\"mask size: {mask.size()}\")\n",
    "        scaled = scaled + mask\n",
    "    attenion = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attenion, v)\n",
    "    return values, attenion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
