{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder from scratch\n",
    "\n",
    "> This project aims to build a transformer encoder archetecture using the Transformer model paper. We will use the following diagram to design the architecture:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/The-Transformer-encoder-diagram.jpg\" >\n",
    "    <figcaption> Fig. 1: The Transformer encoder architecture. </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-head Attention\n",
    "> The key concept we are interested in captioning for the transformer is attention. This is allows for the transformer to attend to different parts of another sequence when making predictions. For large language models (LLM), it is this very important aspect that is crucial for effective performance with language tasks such as sentiment analysis and text summarisation.\n",
    "\n",
    "> We will start by building single-headed attention using the scaled dot product and then later utilise this for multi-headed attention:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/single-head-attention.jpeg\" >\n",
    "    <figcaption> Fig. 2: Single-head attention architecture. </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate the single attention head values and attention.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\"\"\"\n",
    "\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    # print(f\"scaled size: {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        # print(f\"mask size: {mask.size()}\")\n",
    "        scaled = scaled + mask\n",
    "    attenion = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attenion, v)\n",
    "    return values, attenion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "> Now we can build the multi-head attention functionality. The diagram below shows how to construct multi-head attention:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/Multi-head-attention.jpeg\" >\n",
    "    <figcaption> Fig. 3: Multi-head attention architecture. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_input_len, d_model = x.size()\n",
    "        #print(f\"x.size() = {x.size()}\")\n",
    "        qkv = self.qkv_layer(x).reshape(batch_size, max_input_len, self.n_head, 3*self.d_head).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1) #splitting the last dimension into 3 parts so shape is 30x8x200x64\n",
    "        #print(f\"q.size(): {q.size()}, k.size(): {k.size()}, v.size(): {v.size()}\")\n",
    "        values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "        #print(f\"values.size(): {values.size()}, attention.size(): {attention.size()}\")\n",
    "        values = values.reshape(batch_size, max_input_len, self.n_head * self.d_head)\n",
    "        #print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        #print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalisation\n",
    "\n",
    "> For neural networks activation values can get very large in magnitude, causing large gradient steps to be taken when performing back propogation, causing unstable training. This can be avoided by normalising the activations of the hidden layers. There are multiple ways to normalise the activations of the hidden layers but we will opt with Z-score normalization.\n",
    "\n",
    "> Using the layer mean and layer standard deviation and Îµ (to ensure non-zero division), summarised in the following formula:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/layer_norm.PNG\" >\n",
    "    <figcaption> Fig. 4: Layer Normalisation Formula </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalisation(torch.nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(parameters_shape))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = x.mean(dims=dims, keepdim=True)\n",
    "        #print(f\"mean: {mean.size()}\")\n",
    "        var = ((x - mean)**2).mean(dims=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        #print(f\"std: {std.size()}\")\n",
    "        y = (x - mean) / std\n",
    "        #print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y + self.beta\n",
    "        #print(f\"out: {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Layer\n",
    "\n",
    "> This is a postion-wise transformation that consists of linear transformation, ReLU, and another linear transformation. The role and purpose is to process the output from one attention layer in a way to better fit the input for the next attention layer. We will use this layer to essentially gain any aditional information applicable before proceeding to normalisation and add prcoess. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, ffn_hidden)\n",
    "        self.linear2 = nn.Linear(ffn_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Encoder Layer\n",
    "\n",
    "> Now we need to put together the encoder architecture using the functions we have built previously. We will follow the structuring of the signle encoder layer architecture below. \n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: left;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/single-encoder-layer.jpg\" >\n",
    "    <figcaption> Fig. 5: Single Encoder layer structure </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, ffn_hidden, n_heads, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_heads)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm1 = LayerNormalisation(parameters_shape=[d_model])\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, ffn_hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm2 = LayerNormalisation(parameters_shape=[d_model])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual_x = x\n",
    "        #print(f\"---- 1st Attention Step ----\")\n",
    "        x = self.attention(x, mask=None)\n",
    "        #print(f\"---- 1st Dropout Step ----\")\n",
    "        x = self.dropout1(x)\n",
    "        #print(f\"---- 1st Add & Norm Step ----\")\n",
    "        x = self.norm1(x + residual_x)\n",
    "        #print(f\"---- 2nd Attention Step ----\")\n",
    "        x = self.ffn(x)\n",
    "        #print(f\"---- 2nd Dropout Step ----\")\n",
    "        x = self.dropout2(x)\n",
    "        #print(f\"---- 2nd Add & Norm Step ----\")\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Encoder Architecture\n",
    "\n",
    "> As illustrated in the diagram for the encoder, in totality, it is made up of N individual encoder layers, hence the last step for us to do is to concatenate all the encoder layers into one single encoder structure. \n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: left;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/concat-encoder.jpg\" >\n",
    "    <figcaption> Fig. 6: Full Encoder structure </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.model):\n",
    "    def __init__(self, d_model, ffn_hidden, n_heads, dropprob, n_layers):\n",
    "        super().__init__()\n",
    "        # Creating a list of EncoderLayer objects (size = n_layers)\n",
    "        # The * unpacks/destructures the list since python lists are not registered in a nn.Module so *[1,2,3] -> 1,2,3 \n",
    "        # nn.sequential will string together the individual encoder layers in the order they are passed into the constructor\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, n_heads, dropprob) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Parameters_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_head = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_input_len = 200\n",
    "ffn_hidden = 2048\n",
    "n_layers = 5\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden, n_head, drop_prob, n_layers)\n",
    "x = torch.randn((batch_size, max_input_len, d_model))\n",
    "out = encoder(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
