{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder from scratch\n",
    "\n",
    "> This project aims to build a transformer encoder archetecture using the Transformer model paper. We will use the following diagram to design the architecture:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/The-Transformer-encoder-diagram.jpg\" >\n",
    "    <figcaption> Fig. 1: The Transformer encoder architecture. </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-head Attention\n",
    "> The key concept we are interested in captioning for the transformer is attention. This is allows for the transformer to attend to different parts of another sequence when making predictions. For large language models (LLM), it is this very important aspect that is crucial for effective performance with language tasks such as sentiment analysis and text summarisation.\n",
    "\n",
    "> We will start by building single-headed attention using the scaled dot product and then later utilise this for multi-headed attention:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/single-head-attention.jpeg\" >\n",
    "    <figcaption> Fig. 2: Single-head attention architecture. </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate the single attention head values and attention.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\"\"\"\n",
    "\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    # print(f\"scaled size: {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        # print(f\"mask size: {mask.size()}\")\n",
    "        scaled = scaled + mask\n",
    "    attenion = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attenion, v)\n",
    "    return values, attenion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "> Now we can build the multi-head attention functionality. The diagram below shows how to construct multi-head attention:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/Multi-head-attention.jpeg\" >\n",
    "    <figcaption> Fig. 3: Multi-head attention architecture. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_input_len, d_model = x.size()\n",
    "        #print(f\"x.size() = {x.size()}\")\n",
    "        qkv = self.qkv_layer(x).reshape(batch_size, max_input_len, self.n_head, 3*self.d_head).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1) #splitting the last dimension into 3 parts so shape is 30x8x200x64\n",
    "        #print(f\"q.size(): {q.size()}, k.size(): {k.size()}, v.size(): {v.size()}\")\n",
    "        values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "        #print(f\"values.size(): {values.size()}, attention.size(): {attention.size()}\")\n",
    "        values = values.reshape(batch_size, max_input_len, self.n_head * self.d_head)\n",
    "        #print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        #print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalisation\n",
    "\n",
    "> For neural networks activation values can get very large in magnitude, causing large gradient steps to be taken when performing back propogation, causing unstable training. This can be avoided by normalising the activations of the hidden layers. There are multiple ways to normalise the activations of the hidden layers but we will opt with Z-score normalization.\n",
    "\n",
    "> Using the layer mean and layer standard deviation and Îµ (to ensure non-zero division), summarised in the following formula:\n",
    "\n",
    "<style>\n",
    "figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<figure>     \n",
    "    <img src=\"images/layer_norm.PNG\" >\n",
    "    <figcaption> Fig. 4: Layer Normalisation Formula </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalisation(torch.nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(parameters_shape))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = x.mean(dims=dims, keepdim=True)\n",
    "        #print(f\"mean: {mean.size()}\")\n",
    "        var = ((x - mean)**2).mean(dims=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        #print(f\"std: {std.size()}\")\n",
    "        y = (x - mean) / std\n",
    "        #print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y + self.beta\n",
    "        #print(f\"out: {out.size()}\")\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
